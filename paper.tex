\documentclass{article}

\usepackage[hyphens]{url}
\usepackage{fancyvrb}
\usepackage{float}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{marvosym}

\input{defs.tex}

% Document
\begin{document}

%\frontmatter

\title{Logistic regression with a latent binary variable and noisy labels}
%\email{$\{$kkalantar,ihh$\}$@berkeley.edu}}

\maketitle

\section{Model}

Following \cite{BootkrajangKaban2012},
consider a set of $\datapoints$ training data points
$\dataset = \left\{ (\realdata_{\datapoint},\catlabel_{\datapoint}): 1 \leq \datapoint \leq \datapoints \right\}$
where
$\realdata_\datapoint \in \reals^\dimensions$
denote $\dimensions$-dimensional real-valued explanatory data (e.g. gene expression levels)
and
$\catlabel_\datapoint \in \left\{ 0, 1 \ldots \catlabels - 1 \right\}$
denotes a categorical label with $\catlabels$ possible values
(e.g. clinician-assigned label incorporating some degree of uncertainty).

We aim to fit this with a two-stage model,
first regressing the explanatory data $\realdata_\datapoint$ to a latent binary variable representing ground truth
$\binlabel_\datapoint \in \left\{ 0, 1 \right\}$
(e.g. disease state),
then modeling clinical labeling as a categorical variable $\catlabel_\datapoint | \binlabel_\datapoint$
that is conditionally-independent of the explanatory data given the ground truth
\begin{eqnarray*}
P(\binlabel = 1 | \realdata, \weights) & = & \logistic{\weights^T \realdata} \\
P(\catlabel = \catlabelindex | \binlabel = \binlabelindex, \labelerrprobs) & = & \labelerrprob_{\binlabelindex,\catlabelindex}
\end{eqnarray*}
where
$\logistic{x} = \frac{1}{1+e^{-x}}$ is the logistic function,
$\weights \in \reals^\dimensions$ are parameters for the logistic regression model, and
$\labelerrprobs$ are probability parameters for the label observation model.

We put a Laplace double-exponential (Lasso) prior on $\weights$, and a uniform\footnote{
  For identifiability of $\binlabel$, we need to break the symmetry of the Dirichlet prior slightly;
  e.g. by adding a pseudocount of 1 for all $\binlabel \to \catlabel$ mappings that ``agree''.
} Dirichlet prior on each row of $\labelerrprobs$
\begin{eqnarray*}
P(\weights) & \propto & \prod_{\dimension=1}^{\dimensions} \exp(-|\weight^{(\dimension)}|) \\
P(\labelerrprobs) & \propto & \prod_{\binlabelindex \in \{0,1\}} \delta \left( 1 - \sum_{\catlabelindex=0}^{\catlabels-1} \labelerrprob_{\binlabelindex,\catlabelindex} \right)
\end{eqnarray*}

This is equivalent to Section 2.2 of \cite{BootkrajangKaban2012},
with the sum over $j$ in equation (8) of that paper constrained to $j\in\{0,1\}$
instead of $j\in\{0,1\ldots K-1\}$.
The paper derives a conjugate gradient optimization algorithm,
and proves its convergence.

\subsection{Quartile approach}

An alternate model is to use the interpretation of logistic regression where a latent {\em continuous-valued} random variable
(obtained by adding logistically-distributed noise to $\transpose{\weights} \realdata$)
is used to obtain the labels ($\binlabel,\catlabel$),
e.g. with $\catlabel$ corresponding to the quartiles.

I haven't pursued this model, as the assumption that $\catlabel$ corresponds to quartiles of the latent variable underlying
logistic regression seems like a possible misfit to the situation of arbitrarily designated clinical labels
(although, conceivably, my assumption that $\catlabel$ is independent of $\realdata$ given $\binlabel$ is just as bad, or worse).


\section{EM algorithm}

How to use the training data $\dataset$ to fit the weights $\weights$ and probabilities $\labelerrprobs$?
One approach is to use the EM (Expectation Maximization) algorithm \cite{DempsterLairdRubin77},
treating the binary-valued latent variables $\missing = \datasubset{\binlabel}$ as {\em missing data},
the dataset $\dataset = (\allrealdata,\allcatlabels)$ as {\em observed data}
(with inputs $\allrealdata = \datasubset{\realdata}$ and observed labels $\allcatlabels = \datasubset{\catlabel}$),
and the weights and probabilities $\emparams = (\weights,\labelerrprobs)$ as the {\em parameters} to be fit by the algorithm.

The conjugate gradient parameter optimization approach
derived by \cite{BootkrajangKaban2012} may well be superior to the EM method.
However I've outlined the EM approach here for reference.

The joint likelihood including observed and missing data is
\begin{eqnarray*}
  P(\missing,\allcatlabels,\emparams|\allrealdata)
  & = & P(\emparams) P(\missing,\allcatlabels|\emparams,\allrealdata) \\
  & = & P(\weights) P(\labelerrprobs) P(\missing|\weights,\allrealdata) P(\allcatlabels|\labelerrprobs,\missing) \\
  & = & P(\weights) P(\labelerrprobs) \prod_{\datapoint=1}^{\datapoints} P(\binlabel_{\datapoint}|\weights,\realdata_{\datapoint}) P(\catlabel_{\datapoint}|\labelerrprobs,\binlabel_{\datapoint})
\end{eqnarray*}

The marginal likelihood to be maximized, using observed data only, is
\begin{eqnarray*}
  P(\allcatlabels,\emparams|\allrealdata) & = & \sum_{\missing} P(\missing,\allcatlabels,\emparams|\allrealdata) \\
  & = & P(\weights) P(\labelerrprobs) \prod_{\datapoint=1}^{\datapoints} \sum_{\binlabelindex \in \{0,1\}} P(\binlabel_{\datapoint}=\binlabelindex|\weights,\realdata_{\datapoint}) P(\catlabel_{\datapoint}|\labelerrprobs,\binlabel_{\datapoint}=\binlabelindex)
\end{eqnarray*}

At the ($\emiteration$+1)'th iteration, the parameters found by the EM algorithm are given by maximizing the expected log-likelihood
\begin{eqnarray*}
  \emparams^{(\emiteration+1)}
  & = &
  \argmax_{\emparams}\ 
  \emobjective\left(\emparams||\emparams^{(\emiteration)}\right)
  \\
  \emobjective\left(\emparams||\emparams^{(\emiteration)}\right)
  & = & 
  \sum_{\missing} P(\missing | \emparams^{(\emiteration)}, \allrealdata, \allcatlabels) \log P(\missing,\allcatlabels,\emparams|\allrealdata)
  \\
  & = & 
  \log P(\weights) + \log P(\labelerrprobs)
  + \sum_{\missing} P(\missing | \emparams^{(\emiteration)}, \allrealdata, \allcatlabels) \left[ \log P(\missing| \weights, \allrealdata) + \log P(\allcatlabels|\labelerrprobs,\missing) \right]
  \\
  & = & 
  \log P(\weights) + \log P(\labelerrprobs)
  \\ & &
  + \sum_{\datapoint} \sum_{\binlabelindex \in \{0,1\}} P(\binlabel_{\datapoint} = \binlabelindex | \emparams^{(\emiteration)}, \realdata_{\datapoint}, \catlabel_{\datapoint}) \left[ \log P(\binlabel_{\datapoint} = \binlabelindex | \weights, \realdata_{\datapoint}) + \log P(\catlabel_{\datapoint}|\labelerrprobs,\binlabel_{\datapoint}=\binlabelindex) \right]
  \\ & = &
  \emobjective_{\weights}\ +\ \emobjective_{\labelerrprobs}
  \\
  \emobjective_{\weights}
  & = &
  \log P(\weights)
  + \sum_{\datapoint} \left[ (1-\postprob) \log (1 - \logistic{\transpose{\weights} \realdata_{\datapoint}})
  + \postprob \log \logistic{\transpose{\weights} \realdata_{\datapoint}} \right]
  \\
  \emobjective_{\labelerrprobs}
  & = &
  \log P(\labelerrprobs)
  + \sum_{\datapoint} \left[ (1-\postprob) \log \labelerrprob_{0,\catlabel_{\datapoint}} + \postprob \log \labelerrprob_{1,\catlabel_{\datapoint}} \right]
  \\
  \postprob
  & = &
  P(\binlabel_{\datapoint} = 1 | \emparams^{(\emiteration)}, \realdata_{\datapoint}, \catlabel_{\datapoint})
  \\
  P(\binlabel_{\datapoint} = 1 | \emparams, \realdata_{\datapoint}, \catlabel_{\datapoint})
  & = &
  \frac{1}{1 + \frac{P(\catlabel_{\datapoint}, \binlabel_{\datapoint} = 0 | \emparams, \realdata_{\datapoint})}
    {P(\catlabel_{\datapoint}, \binlabel_{\datapoint} = 1 | \emparams, \realdata_{\datapoint})}}
  \\
  & = &
  \frac{1}{1 + \frac{(1 - \logistic{\transpose{\weights} \realdata_{\datapoint}}) \labelerrprob_{0,\catlabel_{\datapoint}}}
    {\logistic{\transpose{\weights} \realdata_{\datapoint}} \labelerrprob_{1,\catlabel_{\datapoint}}}}
\end{eqnarray*}

The maximization of $\emobjective_{\weights}$ w.r.t. $\weights$ is a weighted, Lasso-penalized logistic regression
(the weights being the $\postprob$).

The maximization of $\emobjective_{\labelerrprobs}$ w.r.t. $\labelerrprobs$ should be a matter of counting.

\subsection{Implementation of weighted logistic regression in R}

The maximization of $\emobjective_{\weights}$ w.r.t. $\weights$ can be performed using R's \rcode{glm()} function (generalized linear model regression)
with \rcode{family = binomial(link = "logit")}
(logistic regression is equivalent to binomial-family GLM regression with the ``logit'' link function).

R's GLM-fitter allows {\em weighting} of the training examples;
that is, a weight-augmented dataset $\dataset' = \{ (\realdata'_{\datapoint},\binlabel'_{\datapoint},\freq'_{\datapoint}): 1 \leq \datapoint \leq \datapoints' \}$
where $\freq'_{\datapoint}$ is a {\em weight} (by default 1),
loosely equivalent (when integer-valued) to the number of times datapoint $(\realdata_{\datapoint},\binlabel_{\datapoint})$ was observed, or its frequency.

To implement the M-step in the $(\emiteration+1)$'th iteration of EM,
we construct a weighted pseudo-dataset $\dataset'$ containing $\datapoints' = 2\datapoints$ weighted training examples
(that is, twice as many as the original unweighted dataset $\dataset$).
The first $\datapoints$ are labeled as negatives, the remainder as positives;
the weights are set using $\postprob$, the posterior probability inferences from the previous step of EM.
Specifically, for $1 \leq \datapoint \leq \datapoints$
\begin{eqnarray*}
  \realdata'_{\datapoint} & = & \realdata_{\datapoint} \\
  \binlabel'_{\datapoint} & = & 0 \\
  \freq'_{\datapoint} & = & 1 - \postprob \\
  \realdata'_{\datapoints+\datapoint} & = & \realdata_{\datapoint} \\
  \binlabel'_{\datapoints+\datapoint} & = & 1 \\
  \freq'_{\datapoints+\datapoint} & = & \postprob
\end{eqnarray*}

The weights can be supplied to the R code using using the \rcode{weights} argument to \rcode{glm()}.

The weighted logistic regression fit can then be worked into an R program that implements EM.
Pseudocode for this algorithm is as follows
\begin{itemize}
\item Set $\emparams^{(1)}$ to some ``sensible'' initial values
\item For $\emiteration \in \{ 1, 2, 3 \ldots \}$ do:
\begin{itemize}
\item Calculate $\postprob$ using current $\emparams^{(\emiteration)} = (\weights^{(\emiteration)},\labelerrprobs^{(\emiteration)})$
\item Set $\labelerrprobs^{(\emiteration+1)} \leftarrow \argmax_{\labelerrprobs} ( \emobjective_{\labelerrprobs} )$ by counting \& normalizing
\item Set $\weights^{(\emiteration+1)} \leftarrow \argmax_{\weights} ( \emobjective_{\weights} )$ using \rcode{glm()} with weights
\end{itemize}
\item While $\log \frac{P(\allcatlabels,\emparams^{(\emiteration+1)}|\allrealdata)}{P(\allcatlabels,\emparams^{(\emiteration)}|\allrealdata)}$ is increasing
\end{itemize}

\bibliographystyle{plain}
\bibliography{references}


\end{document}
